{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 103: Introduction to data science <br> Demo \\#5: Pre-processing and storage <br> Author: JRW\n",
    "\n",
    "## Mission\n",
    "This workbook explores some of the challenges that arise once a source of data is identified. The song lyrics database is a nice example because our pipeline interacts with many aspects of the life cycle. Because we are allowed to download one copy of the entire website it is possible to have a complete collection, but this comes with a twist.\n",
    "\n",
    "#### What's nice about a data API?\n",
    "A data API, like those offered by with Twitter, Google, or Facebook, etc., provides convenient access for all kinds of data science projects. These systems of tools provide the opportunity to target specific subsets of data, without the burden of having to store everything and filter down first. However as we know, APIs often come with restrictions and paywalls.\n",
    "\n",
    "#### Could songlyrics.com have an API?\n",
    "Yes, of course, but their web server probably lacks the capacity to handle frequent and complicated requests. This limitation is actually reflected in their terms and conditions in the \"one copy\" policy. This \"one copy\" policy provides full access, similar to a database \"dump\", but also prevents users from developing their own API's. For example, if a user developed an API an d used it multiple times, they could easily violate the conditions by accessing the same piece of data twice. This is too bad! Unfortunately, then, we can't avoid having to collect the entire database before beginning with exploration, analysis and product design, etcetera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, re, string, json, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does our web-scraped data have good API-like functionality?\n",
    "Suppose we want to only check out rock-genre song lyrics. Is our data storage system optimized for this task? No! Since web scraper covers the website alphabetically, we simplistically stored the data in large files by letter. This means the entire a-file would have to be read to get through to the last artist to find their rock-genre songs.\n",
    "\n",
    "#### How can we build good API-like functionality?\n",
    "At the very least, we should probably separate artists into individual files, but perhaps keep the alphabetic index by directories. Additionally, we should probably separate a table of metadata, giving us a roadmap for efficient slicing and dicing.\n",
    "\n",
    "#### What sort of database are we building?\n",
    "Note, that by separating out the artists and metadata, we are making it easier to:\n",
    "\n",
    "1. insert new songs for individual artists, and \n",
    "2. make complex queries for specific data transformations.\n",
    "\n",
    "So, for its flexibility and update speed, our target database format is probably best described as an OLTP system.\n",
    "\n",
    "#### What about when we actually want to use the data for analysis?\n",
    "Since this is an OLTP database, it will not actually be optimized for any specific analysis. So if we want to do something specific with the data, we will have to occasionally call our OLTP system to make specific transformations, which we will store elsewhere, thus creating an updating an OLAP database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data right, the first time\n",
    "Rather than download all of the data as we had planned, it would probably be better to get the structure right upon access. So, here we're starting with our old web scraper, which stores songs by artist in large, alphabetic data files. Let's start out by highlighting where the changes will need to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "####### 0. Create a primary data directory. ###########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "####### 1. Create reverse-lookup for songs by genre ###################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "## go through all of the letters in the alphabet\n",
    "for letter in string.ascii_lowercase:\n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 2. Create the letter-level directory ##########################\n",
    "    #######################################################################\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 3. Initialize a letter-level metadata file ####################\n",
    "    ## create a data file for the current letter\n",
    "    filename = \"songlyrics-\"+letter+\".json\"\n",
    "    fh = open(filename,  \"w\")\n",
    "    fh.close()\n",
    "    #######################################################################\n",
    "    #######################################################################\n",
    "    \n",
    "    ## open and parse the html for the current letter\n",
    "    letterhtml = requests.get(\"http://www.songlyrics.com/\"+letter+\"/\").text\n",
    "    lettersoup = BeautifulSoup(letterhtml, 'lxml')\n",
    "\n",
    "    ## collect the pages for this letter\n",
    "    pages = [\"/\"+letter+\"/\"]\n",
    "    for letterlink in lettersoup.find_all('a'):\n",
    "        ## filter links for letter pages\n",
    "        if letterlink.get(\"href\") is not None and re.search(\"^Page \\d+$\", letterlink.get(\"title\", \"NOTITLE\")):            \n",
    "            pages.append(letterlink['href'])\n",
    "\n",
    "    ## go through the letter pages\n",
    "    for page in pages:        \n",
    "        ## open and parse the html for the current page of this letter\n",
    "        pagehtml = requests.get(\"http://www.songlyrics.com\"+page).text\n",
    "        pagesoup = BeautifulSoup(pagehtml, 'lxml')\n",
    "\n",
    "        ## go through the artists in the page\n",
    "        for pagelink in pagesoup.find_all('a'):\n",
    "            ## filter links for artist pages\n",
    "            if re.search(\"^http://.*?-lyrics/$\",pagelink.get(\"href\", \"NOLINK\")):\n",
    "\n",
    "                #######################################################################                \n",
    "                ####### 4. remove old data structure and hold on to the artist's name #\n",
    "                ## set up data and store artist-level information\n",
    "                data = {\n",
    "                    \"Artist\": pagelink.text,\n",
    "                    \"url\": pagelink['href'],\n",
    "                    \"Songs\": {}\n",
    "                }\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "\n",
    "                #######################################################################\n",
    "                ####### 5. Output artist info to letter-level metadata file ###########\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "\n",
    "                #######################################################################\n",
    "                ####### 6. Create artist-level directory. #############################\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "                #######################################################################\n",
    "                ####### 7. Create an artist-level metadata file #######################\n",
    "                #######################################################################\n",
    "                #######################################################################      \n",
    "                \n",
    "                ## open and parse the html for the current artist on this page\n",
    "                artisthtml = requests.get(data[\"url\"]).text\n",
    "                artistsoup = BeautifulSoup(artisthtml, 'lxml')                        \n",
    "\n",
    "                ## go through the songs of this artist\n",
    "                for songlink in artistsoup.find_all('a'):\n",
    "\n",
    "                    ## filter links for song pages\n",
    "                    if songlink.get(\"itemprop\", \"NOITEMPROP\") == \"url\" and songlink.get(\"title\") is not None:\n",
    "                                                \n",
    "                        #######################################################################\n",
    "                        ############ 8. Hold song title; store info as artist-level metadata ##\n",
    "                        ## store initial song-level information\n",
    "                        title = songlink.text\n",
    "                        data[\"Songs\"][title] = {\"Title\": title}\n",
    "                        data[\"Songs\"][title][\"url\"] = songlink['href']\n",
    "                        #######################################################################\n",
    "                        #######################################################################                        \n",
    "\n",
    "                        ## open and parse the html for the current song by this artist\n",
    "                        songhtml = requests.get(data[\"Songs\"][title][\"url\"]).text\n",
    "                        songsoup = BeautifulSoup(songhtml, 'lxml')\n",
    "\n",
    "                        ## go through paragraphs to find song attributes\n",
    "                        for par in songsoup.find_all(\"p\"):\n",
    "                            if re.search(\": \", par.text):\n",
    "                                pieces = re.split(\": \", par.text)\n",
    "                                key = pieces[0]\n",
    "                                value = \": \".join(pieces[1:len(pieces)])\n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 9. add song attributes to artist-level metadata ##########\n",
    "                                data[\"Songs\"][title][key] = value    \n",
    "                                #######################################################################\n",
    "                                #######################################################################                        \n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 10. add song attributes to reverse song lookup ###########\n",
    "                                #######################################################################\n",
    "                                #######################################################################                                \n",
    "\n",
    "                        #######################################################################                                \n",
    "                        ############ 11. output song metadata to artist-level metadata file ###\n",
    "                        #######################################################################\n",
    "                        #######################################################################                                \n",
    "                                \n",
    "                        ## go through divs to find the one with the song lyrics\n",
    "                        for div in songsoup.find('body').find_all('div'):\n",
    "                            if div.get(\"id\",\"NOCLASS\") == \"songLyricsDiv-outer\":\n",
    "                                \n",
    "                                #######################################################################                                \n",
    "                                ############ 12. output song lyrics as text in artist-level directory #                                \n",
    "                                data[\"Songs\"][title][\"Lyrics\"]=div.text\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "                        \n",
    "                        break\n",
    "                        \n",
    "                #######################################################################\n",
    "                #### 13. remove old data write out ####################################\n",
    "                \n",
    "                ## write out the data for this artist, appending to the end of this letter's file\n",
    "                with open(filename, \"a\") as fh:\n",
    "                    fh.writelines(json.dumps(data)+\"\\n\")\n",
    "                    \n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "                break\n",
    "        break\n",
    "        \n",
    "    break\n",
    "\n",
    "#######################################################################\n",
    "####### 14. Output reverse-lookup for songs by attributes #############\n",
    "#######################################################################\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Making the changes\n",
    "Note that by making a reverse lookup metadata file for artists by associated song attributes, we are actually beginning to look forward towards creating an OLAP database. Why? This is because we are:\n",
    "\n",
    "1. Denormalizing data, i.e., duplicating information,\n",
    "2. making it easy to specific transformations interesting for analysis. \n",
    "\n",
    "Note that throughout this code that we have had to create unique keys for artists and song titles, since using their actual texts as keys is dangerous, on account of difficult characters! Anyway, here's the modified scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "####### 0. Create a primary data directory. ###########################\n",
    "os.system(\"mkdir ./data/\")\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "####### 1. Create objects for reverse-lookup of songs by genre ########\n",
    "songsByAttribute = {}\n",
    "attributeIDs = {}\n",
    "attributes = {}\n",
    "attributeNumbers = {}\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "## go through all of the letters in the alphabet\n",
    "for letter in string.ascii_lowercase:\n",
    "\n",
    "    numartists = 0\n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 2. Create the letter-level directory ##########################\n",
    "    os.system(\"mkdir ./data/\"+letter+\"/\")\n",
    "    #######################################################################\n",
    "    #######################################################################    \n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 3. Initialize a letter-level metadata file ####################\n",
    "    lettermetafile = \"./data/\"+letter+\"/lettermeta.json\"\n",
    "    fh = open(lettermetafile,  \"w\")\n",
    "    fh.close()    \n",
    "    #######################################################################\n",
    "    #######################################################################\n",
    "    \n",
    "    ## open and parse the html for the current letter\n",
    "    letterhtml = requests.get(\"http://www.songlyrics.com/\"+letter+\"/\").text\n",
    "    lettersoup = BeautifulSoup(letterhtml, 'lxml')\n",
    "\n",
    "    ## collect the pages for this letter\n",
    "    pages = [\"/\"+letter+\"/\"]\n",
    "    for letterlink in lettersoup.find_all('a'):\n",
    "        ## filter links for letter pages\n",
    "        if letterlink.get(\"href\") is not None and re.search(\"^Page \\d+$\", letterlink.get(\"title\", \"NOTITLE\")):            \n",
    "            pages.append(letterlink['href'])\n",
    "\n",
    "    ## go through the letter pages\n",
    "    for page in pages:        \n",
    "        ## open and parse the html for the current page of this letter\n",
    "        pagehtml = requests.get(\"http://www.songlyrics.com\"+page).text\n",
    "        pagesoup = BeautifulSoup(pagehtml, 'lxml')\n",
    "\n",
    "        ## go through the artists in the page\n",
    "        for pagelink in pagesoup.find_all('a'):\n",
    "            ## filter links for artist pages\n",
    "            if re.search(\"^http://.*?-lyrics/$\",pagelink.get(\"href\", \"NOLINK\")):\n",
    "                \n",
    "                #######################################################################                \n",
    "                ####### 4. remove old data structure and hold on to the artist's data #\n",
    "                ## keep track of number of artists, songs, and create an ID\n",
    "                numartists += 1\n",
    "                artistID = letter+\"-\"+str(numartists)\n",
    "                numsongs = 0\n",
    "                \n",
    "                artist = pagelink.text\n",
    "                artisturl = pagelink['href']\n",
    "                \n",
    "#                 ## set up data and store artist-level information\n",
    "#                 data = {\n",
    "#                     \"Artist\": pagelink.text,\n",
    "#                     \"url\": pagelink['href'],\n",
    "#                     \"Songs\": {}\n",
    "#                 }                \n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "\n",
    "                #######################################################################                \n",
    "                ####### 5. Output artist info to letter-level metadata file ###########\n",
    "                with open(lettermetafile,  \"a\") as f:\n",
    "                    f.writelines(artistID+\"\\t\"+artist.encode(\"utf-8\")+\"\\t\"+artisturl.encode(\"utf-8\")+\"\\n\")                    \n",
    "                #######################################################################\n",
    "                #######################################################################                    \n",
    "                    \n",
    "                #######################################################################\n",
    "                ####### 6. Create artist-level directory. #############################\n",
    "                os.system(\"mkdir ./data/\"+letter+\"/\"+artistID+\"/\")\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "                #######################################################################\n",
    "                ####### 7. Create an artist-level metadata file #######################\n",
    "                artistmetafile = \"./data/\"+letter+\"/\"+artistID+\"/artistmeta.json\"\n",
    "                fh = open(artistmetafile,  \"w\")\n",
    "                fh.close()               \n",
    "                #######################################################################\n",
    "                #######################################################################                \n",
    "                                \n",
    "                ## open and parse the html for the current artist on this page\n",
    "                ## note we now use the artist's url!\n",
    "                artisthtml = requests.get(artisturl).text\n",
    "                artistsoup = BeautifulSoup(artisthtml, 'lxml')                        \n",
    "\n",
    "                ## go through the songs of this artist\n",
    "                for songlink in artistsoup.find_all('a'):\n",
    "\n",
    "                    ## filter links for song pages\n",
    "                    if songlink.get(\"itemprop\", \"NOITEMPROP\") == \"url\" and songlink.get(\"title\") is not None:                        \n",
    "\n",
    "                        #######################################################################\n",
    "                        ############ 8. Hold song title; store info as artist-level metadata ##\n",
    "                        ## keep track of number of songs and create and ID\n",
    "                        numsongs += 1\n",
    "                        titleID = artistID+\"-\"+str(numsongs)\n",
    "                        \n",
    "                        ## hold on to the song's title\n",
    "                        title = songlink.text\n",
    "                        \n",
    "#                         data[\"Songs\"][title] = {\"Title\": title}\n",
    "#                         data[\"Songs\"][title][\"url\"] = songlink['href']\n",
    "\n",
    "                        data = {\n",
    "                            \"ID\": titleID,\n",
    "                            \"title\": title,\n",
    "                            \"url\": songlink['href']\n",
    "                        }\n",
    "                        #######################################################################\n",
    "                        #######################################################################\n",
    "\n",
    "                        ## open and parse the html for the current song by this artist\n",
    "                        ## note the data format has changed to get the song's url!\n",
    "                        songhtml = requests.get(data[\"url\"]).text\n",
    "                        songsoup = BeautifulSoup(songhtml, 'lxml')\n",
    "\n",
    "                        ## go through paragraphs and get song attributes\n",
    "                        for par in songsoup.find_all(\"p\"):\n",
    "                            if re.search(\": \", par.text):\n",
    "                                pieces = re.split(\": \", par.text)\n",
    "                                key = pieces[0]\n",
    "                                value = \": \".join(pieces[1:len(pieces)])\n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 9. add song attributes to artist-level metadata ##########\n",
    "                                if key != \"Note\":\n",
    "                                    data[key] = value\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "                                \n",
    "                                #######################################################################                                \n",
    "                                ############ 10. add song attributes to reverse song lookup ###########\n",
    "                                if key != \"Note\":\n",
    "                                    attributeNumbers.setdefault(key, 1)\n",
    "                                    attributeIDs.setdefault(key, {})\n",
    "                                    attributes.setdefault(key, {})\n",
    "                                    if not attributeIDs[key].get(value, False):\n",
    "                                        attributeID = key+\"-\"+str(attributeNumbers[key])\n",
    "                                        attributes[key][attributeID] = value\n",
    "                                        attributeIDs[key][value] = attributeID\n",
    "                                        attributeNumbers[key] += 1\n",
    "                                    else:\n",
    "                                        attributeID = attributeIDs[key][value]                                        \n",
    "                                    \n",
    "                                    songsByAttribute.setdefault(key, {})\n",
    "                                    songsByAttribute[key].setdefault(attributeID, {})\n",
    "                                    songsByAttribute[key][attributeID].setdefault(artistID, [])\n",
    "                                    songsByAttribute[key][attributeID][artistID].append(titleID)\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "\n",
    "                        #######################################################################                                \n",
    "                        ############ 11. output song metadata to artist-level metadata file ###\n",
    "                        with open(artistmetafile,  \"a\") as f:\n",
    "                            f.writelines(json.dumps(data)+\"\\n\")\n",
    "                        #######################################################################\n",
    "                        #######################################################################                            \n",
    "                                \n",
    "                        ## go through divs to find the one with the song lyrics\n",
    "                        for div in songsoup.find('body').find_all('div'):\n",
    "                            if div.get(\"id\",\"NOCLASS\") == \"songLyricsDiv-outer\":\n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 12. output song lyrics as text in artist-level directory #\n",
    "                                with open(\"./data/\"+letter+\"/\"+artistID+\"/\"+titleID+\".txt\", \"w\") as f:\n",
    "                                    f.writelines(div.text.encode(\"utf-8\")+\"\\n\")\n",
    "                                \n",
    "#                                 data[\"Songs\"][title][\"Lyrics\"]=div.text\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "\n",
    "                                break\n",
    "            \n",
    "                    ## now, only break after 10 songs by an artist\n",
    "                    if numsongs >= 1:\n",
    "                        break\n",
    "                        \n",
    "                #######################################################################\n",
    "                #### 13. remove old data write out ####################################\n",
    "#                 ## write out the data for this artist, appending to the end of this letter's file\n",
    "#                 with open(filename, \"a\") as fh:\n",
    "#                     fh.writelines(json.dumps(data)+\"\\n\")\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "            ## now, only break if this is the tenth artist of this letter!\n",
    "            if numartists >= 1:\n",
    "                break\n",
    "        \n",
    "        ## this stops us after one page of each letter\n",
    "        break\n",
    "        \n",
    "    ## this stops us after one letter in the alphabet\n",
    "#     break\n",
    "\n",
    "#######################################################################\n",
    "####### 14. Output reverse-lookup for songs by genre ##################\n",
    "os.system(\"mkdir ./data/Genre/\")\n",
    "fh = open(\"./data/Genre/attributeIDs.txt\", \"w\")\n",
    "for attributeID in songsByAttribute[\"Genre\"]:\n",
    "    fh.writelines(attributeID+\"\\t\"+attributes[\"Genre\"][attributeID].encode(\"utf-8\")+\"\\n\")\n",
    "    with open(\"./data/Genre/\"+attributeID+\".json\", \"w\") as f:\n",
    "        f.writelines(json.dumps(songsByAttribute[\"Genre\"][attributeID])+\"\\n\")\n",
    "fh.close()\n",
    "#######################################################################\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing our database\n",
    "As mentioned, the goal here is to replicate an API-like service for a local database. Let's work on our goal of being able to access data by genre. To do this, we'll make a function that reads the appropriate reverse-lookup file and finds all songs titles/artists with the desired genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genres = {}\n",
    "with open(\"./data/Genre/attributeIDs.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        ID, genre = re.split(\"\\t\", line)\n",
    "        genres[genre] = ID\n",
    "\n",
    "def genreSongs(genre):\n",
    "    with open(\"./data/Genre/\"+genres[genre]+\".json\") as f:\n",
    "        genredata = json.loads(f.read())\n",
    "    data = []\n",
    "    for artistID in genredata:\n",
    "        letter = artistID[0]\n",
    "        songs = genredata[artistID]\n",
    "        with open(\"./data/\"+letter+\"/\"+artistID+\"/artistmeta.json\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                songmeta = json.loads(line)\n",
    "                if songmeta[\"ID\"] in songs:\n",
    "                    data.append((songmeta[\"Artist\"], songmeta[\"title\"]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: A\n",
      "Song: Sing-A-Long\n",
      "\n",
      "Artist: X\n",
      "Song: 4th of July\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for artist, song in genreSongs(\"Rock\"):\n",
    "    print(\"Artist: \"+artist)\n",
    "    print(\"Song: \"+song)\n",
    "    print    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to go from here?\n",
    "This has really just been the tip of the iceberg! To create a truly functional API-like utility we will need to make a whole suite of functions that can slice and dice our data. There are many more attributes besides Genre, and we may just want some basic stuff, too, that can look up to find all of the songs by an artist or within an album."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
